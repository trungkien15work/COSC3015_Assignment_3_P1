{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 2&3\n",
    "#### Student Name: XXXX XXXX\n",
    "#### Student ID: 000000\n",
    "\n",
    "Date: XXXX\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "\n",
    "## Introduction\n",
    "You should give a brief information of this assessment task here.\n",
    "\n",
    "<span style=\"color: red\"> Note that this is a sample notebook only. You will need to fill in the proper markdown and code blocks. You might also want to make necessary changes to the structure to meet your own needs. Note also that any generic comments written in this notebook are to be removed and replace with your own words.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of directories containing the text files\n",
    "directories = [\n",
    "    r\"C:\\Users\\Kien\\Downloads\\assess3_data\\data\\Engineering\",\n",
    "    r\"C:\\Users\\Kien\\Downloads\\assess3_data\\data\\Healthcare_Nursing\",\n",
    "    r\"C:\\Users\\Kien\\Downloads\\assess3_data\\data\\Accounting_Finance\",\n",
    "    r\"C:\\Users\\Kien\\Downloads\\assess3_data\\data\\Sales\"\n",
    "]\n",
    "\n",
    "# Path to the stop words file\n",
    "stopwords_file = r\"C:\\Users\\Kien\\Downloads\\assess3_data\\stopwords_en.txt\"\n",
    "\n",
    "# Load stop words\n",
    "with open(stopwords_file, 'r') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text, stopwords, term_frequencies, top_50_terms):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation (if necessary, but the regex handles this too)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize using the specific regular expression\n",
    "    tokens = re.findall(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\", text)\n",
    "    # Remove words with length less than 2\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    # Remove words that appear only once in the document collection\n",
    "    tokens = [word for word in tokens if term_frequencies[word] > 1]\n",
    "    # Remove the top 50 most frequent words\n",
    "    tokens = [word for word in tokens if word not in top_50_terms]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "# Collect term frequencies across all documents\n",
    "term_frequencies = Counter()\n",
    "documents = []\n",
    "\n",
    "# Build the term frequency dictionary\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding= 'unicode_escape') as file:\n",
    "                content = file.read()\n",
    "                description = content.split(\"Description: \")[1].strip()\n",
    "                # Convert description to lowercase and tokenize\n",
    "                tokens = description.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "                term_frequencies.update(tokens)\n",
    "                documents.append(tokens)\n",
    "\n",
    "# Identify the top 50 most frequent words\n",
    "top_50_terms = {word for word, _ in term_frequencies.most_common(50)}\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Preprocess the text and store the results\n",
    "for directory in directories:\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r', encoding= 'unicode_escape') as file:\n",
    "                content = file.read()\n",
    "                title = content.split(\"Title: \")[1].split(\"\\n\")[0]\n",
    "                webindex = content.split(\"Webindex: \")[1].split(\"\\n\")[0]\n",
    "                description = content.split(\"Description: \")[1].strip()\n",
    "                # Pre-process the Description\n",
    "                preprocessed_description = preprocess_text(description, stopwords, term_frequencies, top_50_terms)\n",
    "                # Store the result\n",
    "                results.append({\n",
    "                    'Title': title,\n",
    "                    'Webindex': webindex,\n",
    "                    'Description': preprocessed_description\n",
    "                })\n",
    "\n",
    "# # Print the results\n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Job Advertisement Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary from the file\n",
    "vocab_dict = {}\n",
    "with open('vocab.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word, idx = line.strip().split(':')\n",
    "        vocab_dict[word] = int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer with the custom vocabulary\n",
    "c_vector = CountVectorizer(vocabulary=vocab_dict)\n",
    "\n",
    "# Extract the preprocessed descriptions and corresponding webindex from the results\n",
    "descriptions = [result['Description'] for result in results]\n",
    "webindexes = [result['Webindex'] for result in results]\n",
    "\n",
    "# Fit and transform the descriptions using the c_vector\n",
    "X_c = c_vector.fit_transform(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TfidfVectorizer with the custom vocabulary\n",
    "tfidf_vector = TfidfVectorizer(vocabulary=vocab_dict)\n",
    "\n",
    "# Fit and transform the descriptions using the TfidfVectorizer\n",
    "X_tfidf = tfidf_vector.fit_transform(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unweighted Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the count vector representation as per spectification.\n",
    "- count_vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file to write the sparse count vector representation\n",
    "output_file_path = 'count_vectors.txt'\n",
    "with open(output_file_path, 'w') as file:\n",
    "    # Iterate over each description and its corresponding webindex\n",
    "    for idx, webindex in enumerate(webindexes):\n",
    "        # Get the non-zero elements of the sparse matrix row\n",
    "        sparse_row = X_c[idx]\n",
    "        non_zero_indices = sparse_row.nonzero()[1]\n",
    "        sparse_representation = []\n",
    "        \n",
    "        for word_idx in non_zero_indices:\n",
    "            word_count = sparse_row[0, word_idx]\n",
    "            sparse_representation.append(f\"{word_idx}:{word_count}\")\n",
    "        \n",
    "        # Format the line\n",
    "        line = f\"#{webindex},\" + ','.join(sparse_representation) + '\\n'\n",
    "        file.write(line)\n",
    "\n",
    "print(f\"Count vectors exported to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Job Advertisement Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on buidling classification models based on different document feature represetations. \n",
    "Detailed comparsions and evaluations on different models to answer each question as per specification. \n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to perform the task...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Give a short summary and anything you would like to talk about the assessment tasks here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couple of notes for all code blocks in this notebook\n",
    "- please provide proper comment on your code\n",
    "- Please re-start and run all cells to make sure codes are runable and include your output in the submission.   \n",
    "<span style=\"color: red\"> This markdown block can be removed once the task is completed. </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
