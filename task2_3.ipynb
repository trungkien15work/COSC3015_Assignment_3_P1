{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 2&3\n",
    "#### Student Name: Dao Sy Trung Kien\n",
    "#### Student ID: S3979613\n",
    "\n",
    "Date: 04/09/2024\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "\n",
    "## Introduction\n",
    "In tasks 2 and 3, students need to create feature vectors for job advertisement descriptions and titles. Then, using the created vectors, build machine learning models to classify the category of a job advertisement text. Students should provide answers to two questions:\n",
    "Q1: Language model comparisons\n",
    "Q2: Does more information provide higher accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Job Advertisement Descriptions and Titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Vocabulary from Vocab Files and Extract Title, Description, and Webindex from Preprocessed Job Advertisement File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Load the description vocabulary from the vocab.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_des = {}\n",
    "with open('vocab.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word, idx = line.strip().split(':')\n",
    "        vocab_des[word] = int(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Load the title vocabulary from the vocab_title.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_title = {}\n",
    "with open('vocab_title.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        word, idx = line.strip().split(':')\n",
    "        vocab_title[word] = int(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Extract descriptions, titles, and webindexes from Preprocessed Job Advertisement File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists to store the descriptions, titles, labels, and webindexes\n",
    "descriptions = []\n",
    "titles = []\n",
    "webindexes = []\n",
    "labels = []\n",
    "all_doc = []\n",
    "\n",
    "# Open and read the preprocessed_job_ads.txt file\n",
    "with open('preprocessed_job_ads.txt', 'r') as file:\n",
    "    # First, we skip the header\n",
    "    next(file)\n",
    "    # Next iterate through each line\n",
    "    for line in file:\n",
    "        # Split the line by commas\n",
    "        fields = line.strip().split(', ')\n",
    "        # Extract the 'title', 'webindex', 'labels', and 'description' fields\n",
    "        if len(fields) >= 5:\n",
    "            title = fields[1]\n",
    "            webindex = fields[2]\n",
    "            description = fields[4]\n",
    "            label = fields[5]\n",
    "            # Add the title, webindex, label, and description to the respective lists\n",
    "            titles.append(title)\n",
    "            webindexes.append(webindex)\n",
    "            descriptions.append(description)\n",
    "            labels.append(label)\n",
    "            all_doc.append(title)\n",
    "            all_doc.append(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Descriptions Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object with the vocabulary from the vocab dictionary\n",
    "c_vec_des = CountVectorizer(vocabulary=vocab_des)\n",
    "\n",
    "# Fit and transform descriptions using the c_vec_des\n",
    "X_c_des = c_vec_des.fit_transform(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Titles Count Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object with the vocabulary from the vocab_title dictionary\n",
    "c_vec_title = CountVectorizer(vocabulary=vocab_title)\n",
    "\n",
    "# Fit and transform titles using the c_vec_title\n",
    "X_c_title = c_vec_title.fit_transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Generating TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Desccriptions TF-IDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer object with the vocabulary from the vocab dictionary\n",
    "tfidf_vec_des = TfidfVectorizer(vocabulary=vocab_des)\n",
    "\n",
    "# Fit and transform descriptions using the TfidfVectorizer\n",
    "X_tfidf_des = tfidf_vec_des.fit_transform(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Titles TF-IDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer object with the vocabulary from the vocab_title dictionary\n",
    "tfidf_vec_title = TfidfVectorizer(vocabulary=vocab_title)\n",
    "\n",
    "# Fit and transform titles using the TfidfVectorizer\n",
    "X_tfidf_title = tfidf_vec_title.fit_transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Generating One-hot Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Descriptions One-hot Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Binary CountVectorizer object with the vocabulary from the vocab dictionary\n",
    "one_hot_vec_des = CountVectorizer(vocabulary=vocab_des, binary=True)\n",
    "\n",
    "# Fit and transform the descriptions using the one_hot_vector\n",
    "X_one_des = one_hot_vec_des.fit_transform(descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Titles One-hot Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Binary CountVectorizer object with the vocabulary from the vocab_title dictionary\n",
    "one_hot_vec_title = CountVectorizer(vocabulary=vocab_title, binary=True)\n",
    "\n",
    "# Fit and transform the descriptions using the CountVectorizer\n",
    "X_one_tilte = one_hot_vec_title.fit_transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Generating Word2Vec Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model on the tokenized descriptions\n",
    "word2vecdes_model = Word2Vec(sentences=descriptions, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Word2Vec model on the tokenized titles\n",
    "word2vectitle_model = Word2Vec(sentences=titles, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Saving the Vector Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file for writing the word vectors\n",
    "output_file = 'count_vectors.txt'\n",
    "with open(output_file, 'w') as file:\n",
    "    # Iterate over each description and its corresponding webindex\n",
    "    for idx, webindex in enumerate(webindexes):\n",
    "        # Get the sparse representation of the description\n",
    "        sparse_row = X_c_des[idx]\n",
    "        non_zero_indices = sparse_row.nonzero()[1]\n",
    "        sparse_represent = []\n",
    "        for word_idx in non_zero_indices:\n",
    "            word_count = sparse_row[0, word_idx]\n",
    "            sparse_represent.append(f\"{word_idx}:{word_count}\")\n",
    "        # Format line and write to the file\n",
    "        line = f\"#{webindex},\" + ','.join(sparse_represent) + '\\n'\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Job Advertisement Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on buidling classification models based on different document feature represetations. \n",
    "Detailed comparsions and evaluations on different models to answer each question as per specification. \n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to perform the task...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Give a short summary and anything you would like to talk about the assessment tasks here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couple of notes for all code blocks in this notebook\n",
    "- please provide proper comment on your code\n",
    "- Please re-start and run all cells to make sure codes are runable and include your output in the submission.   \n",
    "<span style=\"color: red\"> This markdown block can be removed once the task is completed. </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
