{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Hong Thai Ngoc Ha, Dao Sy Trung Kien, Dao Quang Minh\n",
    "#### Student ID: S4060340, S3979613, S4015939\n",
    "\n",
    "Date: 04/09/2024\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* os\n",
    "* nltk\n",
    "\n",
    "## Introduction\n",
    "This is the basic text preprocessing part of the project to get the brief information of job advertisements. This part focuses on preprocessing the 'Description' part of the collection of advertisements. To make sure the information is ready for the next steps, every required step for text preprocessing is absolutely performed clearly in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kien\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import all the necessary libaries\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "The dataset that is provided contains multiple folders with a collection of job advertisements. For each job advertisement, it is stored in the TXT file format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get the information of job advertisement\n",
    "def get_job_infors(root_folder):\n",
    "    job_infos = [] # Create an array to store all the information of advertisement\n",
    "    # Get the id from filename\n",
    "    id_pattern = re.compile(r'Job_(\\d+)\\.txt')\n",
    "    target_subfolders = {'Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales'}\n",
    "    # Go through all the file in each folder\n",
    "    for foldername, subfolders, filenames in os.walk(root_folder):\n",
    "        # Ensure to get all the data with the right folders\n",
    "        current_folder = os.path.basename(foldername)\n",
    "        if current_folder in target_subfolders:\n",
    "            for filename in filenames: # For every filename in the filenames get from the folder\n",
    "                if filename.endswith('.txt'): # If the filename contian '.txt', get that file\n",
    "                    job_id_match = id_pattern.search(filename) # Make sure the file match with the name format (Job_(+d).txt)\n",
    "                    if job_id_match: # If the format is match, get the jobId in the filename\n",
    "                        job_id = job_id_match.group(1)\n",
    "                    else: # Else set 'Unknown'\n",
    "                        job_id = 'Unknown'\n",
    "\n",
    "                    # Get the information in the filename\n",
    "                    file_path = os.path.join(foldername, filename)\n",
    "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                        content = file.read()\n",
    "\n",
    "                        # Extracting specific fields from the content\n",
    "                        title = re.search(r'Title:\\s*(.*)', content)\n",
    "                        webindex = re.search(r'Webindex:\\s*(.*)', content)\n",
    "                        company = re.search(r'Company:\\s*(.*)', content)\n",
    "                        description = re.search(r'Description:\\s*(.*)', content)\n",
    "                        \n",
    "                        # Put the information into the easiest format for next steps\n",
    "                        job_info = {\n",
    "                            'ID': job_id,\n",
    "                            'Title': title.group(1).strip() if title else 'Unknown',\n",
    "                            'Webindex': webindex.group(1).strip() if webindex else 'Unknown',\n",
    "                            'Company': company.group(1).strip() if company else 'Unknown',\n",
    "                            'Description': description.group(1).strip() if description else 'Unknown',\n",
    "                            'Label': os.path.basename(foldername) # Get folder name as Label\n",
    "                        }\n",
    "\n",
    "                        job_infos.append(job_info)\n",
    "\n",
    "    return job_infos\n",
    "\n",
    "# Specify the root folder that contain all the job_ads folders\n",
    "root_folder = r'.\\.'\n",
    "job_infors = get_job_infors(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the description of the job_infors into an array\n",
    "descriptions = []\n",
    "for infor in job_infors:\n",
    "    descriptions.append(infor['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stopwords list\n",
    "with open(r'.\\stopwords_en.txt', 'r') as file:\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "In this step, we started to perform all of the text preprocessing steps to complete all the requirements that were provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Tokenization\n",
    "In the beginning, we tokenize all the descriptions with the pattern that was provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization with the giving pattern\n",
    "tokens = []\n",
    "pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "tokenizer = RegexpTokenizer(pattern) \n",
    "for description in descriptions: # For every description in the collection array of description\n",
    "    token = tokenizer.tokenize(description) # Tokenize that description\n",
    "    tokens.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all the words into lower format\n",
    "tokens_lower = []\n",
    "for tokene in tokens: # For every token array_list in the tokens\n",
    "    token_list = [] # Create array token_list to store all tokens of 1 list\n",
    "    for token in tokene: # For every token in the token array_list \n",
    "        token_list.append(token.lower()) # Lower the token and put it back to the token_list\n",
    "    tokens_lower.append(token_list) # Get all the token_list into collection of token_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Remove words with length less than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_length = []\n",
    "for tokens in tokens_lower:\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "        if len(token) >= 2: # If length of the word high than 2, keep the word\n",
    "            token_list.append(token)\n",
    "    tokens_length.append(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Remove stopwords using the provided stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_stopwords = []\n",
    "for tokens in tokens_length:\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords: # If the word not in stopwords list, keep the word\n",
    "            token_list.append(token)\n",
    "    tokens_without_stopwords.append(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Remove the word that appears only once in the document collection, based on term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency = defaultdict(int) # Create a dictonary that contain the frequency of term\n",
    "for tokens in tokens_without_stopwords:\n",
    "    for token in tokens: \n",
    "        term_frequency[token] += 1 # +1 for the word if it appear\n",
    "\n",
    "tokens_more_than_1 = []\n",
    "token_len = []\n",
    "for tokens in tokens_without_stopwords:\n",
    "    tokens_filtered_freq = []\n",
    "    for token in tokens:\n",
    "        if term_frequency[token] > 1: # If the term_frequency higher than 1, keep the word\n",
    "            tokens_filtered_freq.append(token)\n",
    "    tokens_more_than_1.append(tokens_filtered_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f. Remove the top 50 most frequent words based on document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequency = defaultdict(int) # Create a dictonary that contain the document frequency\n",
    "\n",
    "for document in tokens_more_than_1:\n",
    "    unique_document_word = set(document) # Get all the word in a document unique\n",
    "    for word in unique_document_word:\n",
    "        document_frequency[word] += 1 # +1 for everytime the word appear in 1 document\n",
    "\n",
    "more_than_50 = []\n",
    "for word, count in document_frequency.items():\n",
    "    more_than_50.append((word,count)) # Append the word and document_frequency count into more_than_50\n",
    "    more_than_50.sort(key=lambda x: x[1], reverse=True) # Sort the more_than_50 from highest to lowest\n",
    "    if len(more_than_50) > 50: # If this array length more than 50\n",
    "        more_than_50.pop() # Remove the lowest count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After get the list of highest document frequency, remove it in the tokens\n",
    "final_list = []\n",
    "for tokens in tokens_more_than_1:\n",
    "    token_list = []\n",
    "    for token in tokens:\n",
    "        if token not in more_than_50: # If the word not in more_than_50, keep the word\n",
    "            token_list.append(token)\n",
    "    final_list.append(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### g. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_list = [[lemmatizer.lemmatize(token) for token in tokens] for tokens in final_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After finishing the text pre-processing steps, get all the processed description back to job_infors\n",
    "for i, description in enumerate(lemmatized_list):\n",
    "    if i < len(job_infors):\n",
    "        job_infors[i]['Description'] = description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Title vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the title of the job_infors into an array\n",
    "titles = []\n",
    "for infor in job_infors:\n",
    "    titles.append(infor['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "tokenizer = RegexpTokenizer(pattern) \n",
    "for title in titles: # For every title in the collection array of title\n",
    "    token = tokenizer.tokenize(title) # Tokenize that title\n",
    "    tokens.append(token)\n",
    "\n",
    "# Turn every token into lowercase\n",
    "tokens_lower_list = [[token.lower() for token in tokene] for tokene in tokens]\n",
    "# Remove the stopwords\n",
    "tokens_title_without_stopwords = [[token for token in tokens if token not in stopwords] for tokens in tokens_lower_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After finishing the text pre-processing steps, get all the processed titles back to job_infors\n",
    "for i, title in enumerate(tokens_title_without_stopwords):\n",
    "    if i < len(job_infors):\n",
    "        job_infors[i]['Title'] = title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "After finishing the text preprocessing for the descriptions in job advertisements, we save all the job ads into the preprocessed_job_ads.txt file and vocab.txt for vocabulary in the descriptions. We also create the vocab_title.txt for our following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export the information of job advertisement after pre-processing\n",
    "def save_job_info(job_infors):\n",
    "    header = \"ID,  Title,  Webindex,  Company,  Description,  Label\\n\"\n",
    "\n",
    "    lines = [header]\n",
    "\n",
    "    for job_info in job_infors:\n",
    "        # Join all the words into single string in Description\n",
    "        sentence = ' '.join(job_info['Description'])\n",
    "        job_info['Description'] = sentence\n",
    "        # Join all the words into single string in Title\n",
    "        sentence_title = ' '.join(job_info['Title'])\n",
    "        job_info['Title'] = sentence_title\n",
    "        # Create the file's content performance\n",
    "        line = f\"{job_info['ID']},  {job_info['Title']},  {job_info['Webindex']},  {job_info['Company']},  {job_info['Description']},  {job_info['Label']}\\n\"\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = save_job_info(job_infors)\n",
    "# Write the new file based on the content\n",
    "with open('preprocessed_job_ads.txt', 'w', encoding='utf-8') as output_file:\n",
    "    output_file.writelines(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab.txt file to store all Description vocabulary\n",
    "vocabulary = []\n",
    "for tokens in final_list:\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "vocabulary.sort() # Sort to preform from A-Z\n",
    "ids = 0\n",
    "\n",
    "with open('vocab.txt', 'w') as file:\n",
    "    for word in vocabulary:\n",
    "        file.write(f'{word}:{ids}\\n') # Included Id for each vocabulary\n",
    "        ids+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab_title.txt file to store all Title vocabulary\n",
    "vocabulary_title = []\n",
    "for tokens in tokens_title_without_stopwords:\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary_title:\n",
    "            vocabulary_title.append(token)\n",
    "vocabulary_title.sort() # Sort to preform from A-Z\n",
    "ids = 0\n",
    "\n",
    "with open('vocab_title.txt', 'w') as file:\n",
    "    for word in vocabulary_title:\n",
    "        file.write(f'{word}:{ids}\\n') # Included Id for each vocabulary\n",
    "        ids+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "The job advertising' descriptions were successfully prepared for additional analysis and classification by completing all preprocessing tasks. Tokenization, lowercasing, and the removal of stop words, short words, uncommon keywords, and often used words have helped to focus the description section's attention on the relevant information.  The vocabulary of the description and the summary of job advertisement are provided for further tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
